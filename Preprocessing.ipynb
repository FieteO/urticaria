{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7e7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, udf\n",
    "\n",
    "import spacy\n",
    "import gensim\n",
    "from tika import parser\n",
    "import pdfplumber\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction import text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "   \n",
    "df = pd.read_csv('./retrieval/Data/ImportFileList.csv')\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Uticaria_Spark\").getOrCreate()\n",
    "\n",
    "urticaria_urls_rows = df.loc[:, ['Name', 'path']].values.tolist()\n",
    "\n",
    "# create a Pandas dataframe of Urticaria articles incl. path\n",
    "urticaria_path_pd = pd.DataFrame(urticaria_urls_rows, columns=['article', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f343eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frezzolini 2006</td>\n",
       "      <td>./CAPTUM/Basophil/dsDNA/Frezzolini 2006.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hatada 2013</td>\n",
       "      <td>./CAPTUM/Basophil/dsDNA/Hatada 2013.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kolkhir 2020</td>\n",
       "      <td>./CAPTUM/Basophil/Antihistamine/Kolkhir 2020.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magen 2014</td>\n",
       "      <td>./CAPTUM/Basophil/Antihistamine/Magen 2014.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grattan 2000</td>\n",
       "      <td>./CAPTUM/Basophil/Antihistamine/Grattan 2000.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article                                              path\n",
       "0  Frezzolini 2006       ./CAPTUM/Basophil/dsDNA/Frezzolini 2006.pdf\n",
       "1      Hatada 2013           ./CAPTUM/Basophil/dsDNA/Hatada 2013.pdf\n",
       "2     Kolkhir 2020  ./CAPTUM/Basophil/Antihistamine/Kolkhir 2020.pdf\n",
       "3       Magen 2014    ./CAPTUM/Basophil/Antihistamine/Magen 2014.pdf\n",
       "4     Grattan 2000  ./CAPTUM/Basophil/Antihistamine/Grattan 2000.pdf"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urticaria_path_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ab7e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urticaria_urls = spark.createDataFrame(urticaria_path_pd).repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dd3710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arturgergert/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/pandas/functions.py:386: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"in the future releases. See SPARK-28264 for more details.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "@udf('string')\n",
    "def extract_content(path: str) ->str:\n",
    "    try:\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            raw = pdf.pages[1]\n",
    "            raw_text = raw.extract_text()\n",
    "        return raw_text\n",
    "        #raw = parser.from_file(path)\n",
    "        #return raw['content']\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "@udf('string')\n",
    "def extract_meta(path: str) ->str:\n",
    "\n",
    "    try:\n",
    "        raw = parser.from_file(path)\n",
    "        return raw['metadata']\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n",
    "\n",
    "def not_header(line):\n",
    "    # as we're consolidating broken lines into paragraphs, \n",
    "    # we want to make sure not to include headers\n",
    "    return not line.isupper()\n",
    "\n",
    "def extract_statements(nlp, text):\n",
    "  \n",
    "    \"\"\"\n",
    "  Extracting desease statements from raw text by removing junk, URLs, etc.\n",
    "  We group consecutive lines into paragraphs and use spacy to parse sentences.\n",
    "    \"\"\"\n",
    "  \n",
    "  # remove non ASCII characters\n",
    "  \n",
    "    text = remove_non_ascii(text)\n",
    "  \n",
    "    lines = []\n",
    "    prev = \"\"\n",
    "    for line in text.split('\\n'):\n",
    "    # aggregate consecutive lines where text may be broken down\n",
    "    # only if next line starts with a space or previous does not end with dot.\n",
    "        if(line.startswith(' ') or not prev.endswith('.')):\n",
    "            prev = prev + ' ' + line\n",
    "        else:\n",
    "            # new paragraph\n",
    "            lines.append(prev)\n",
    "            prev = line\n",
    "        \n",
    "  # don't forget left-over paragraph\n",
    "    lines.append(prev)\n",
    "\n",
    "    # clean paragraphs from extra space, unwanted characters, urls, etc.\n",
    "    # best effort clean up, consider a more versatile cleaner\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "\n",
    "      # removing header number\n",
    "      line = re.sub(r'^\\s?\\d+(.*)$', r'\\1', line)\n",
    "      # removing trailing spaces\n",
    "      line = line.strip()\n",
    "      # words may be split between lines, ensure we link them back together\n",
    "      line = re.sub('\\s?-\\s?', '-', line)\n",
    "      # remove space prior to punctuation\n",
    "      line = re.sub(r'\\s?([,:;\\.])', r'\\1', line)\n",
    "      # remove figures that are not relevant to grammatical structure\n",
    "      line = re.sub(r'\\d{5,}', r' ', line)\n",
    "      # remove mentions of URLs\n",
    "      line = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', r' ', line)\n",
    "      # remove multiple spaces\n",
    "      line = re.sub('\\s+', ' ', line)\n",
    "\n",
    "      # split paragraphs into well defined sentences using spacy\n",
    "      for part in list(nlp(line).sents):\n",
    "        sentences.append(str(part).strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "@pandas_udf('array<string>', PandasUDFType.SCALAR_ITER)\n",
    "def extract_statements_udf(content_series_iter):\n",
    "    \"\"\"\n",
    "    as loading a spacy model takes time, we certainly do not want to load model for each record to process\n",
    "    we load model only once and apply it to each batch of content this executor is responsible for\n",
    "    \"\"\"\n",
    "\n",
    "    # load spacy model\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "    # cleanse and tokenize a batch of PDF content \n",
    "    for content_series in content_series_iter:\n",
    "        yield content_series.map(lambda x: extract_statements(nlp, x))\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    gen = gensim.utils.simple_preprocess(sentence, deacc=True)\n",
    "    return ' '.join(gen)\n",
    "\n",
    "def lemmatize(nlp, text):\n",
    "  \n",
    "    # parse sentence using spacy\n",
    "    doc = nlp(text) \n",
    "\n",
    "    # convert words into their simplest form (singular, present form, etc.)\n",
    "    lemma = []\n",
    "    for token in doc:\n",
    "        if (token.lemma_ not in ['-PRON-']):\n",
    "            lemma.append(token.lemma_)\n",
    "\n",
    "    return tokenize(' '.join(lemma))\n",
    "\n",
    "@pandas_udf('string', PandasUDFType.SCALAR_ITER)\n",
    "def lemma(content_series_iter):\n",
    "    \"\"\"\n",
    "    as loading a spacy model takes time, we certainly do not want to load model for each record to process\n",
    "    we load model only once and apply it to each batch of content this executor is responsible for\n",
    "    \"\"\"\n",
    "\n",
    "    # load spacy model\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "    # lemmatize a batch of text content into sentences\n",
    "    for content_series in content_series_iter:\n",
    "        yield content_series.map(lambda x: lemmatize(nlp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2583b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9361609b0c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0murticaria_articles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cache PDFs\n",
    "urticaria_articles = urticaria_urls.withColumn('content', extract_content(F.col('path'))) \\\n",
    "    .withColumn('meta', extract_meta(F.col('path'))) \\\n",
    "    .filter(F.length(F.col('content')) > 0) \\\n",
    "    .cache()\n",
    "urticaria_articles.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0baef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e03856",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7511ba7c5a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'statement'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0murticaria_statements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/env_Urticaria/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nAttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/Cellar/apache-spark/3.1.1/libexec/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "urticaria_statements = urticaria_articles.withColumn('statements', extract_statements_udf(F.col('content'))) \\\n",
    "    .withColumn('statement', F.explode(F.col('statements'))) \\\n",
    "    .filter(F.length(F.col('statement')) > 100) \\\n",
    "    .select('article', 'statement') \\\n",
    "    .cache()\n",
    "urticaria_statements.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479e9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "urticaria_lemma = urticaria_statements.withColumn('lemma', lemma(F.col('statement'))) \\\n",
    "    .select('article', 'statement', 'lemma')\n",
    "\n",
    "urticaria = urticaria_lemma.select(\"article\", \"statement\", \"lemma\").toPandas()\n",
    "\n",
    "urticaria.to_csv('./retrieval/Data/ProcessedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af15f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Urticaria",
   "language": "python",
   "name": "env_urticaria"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
