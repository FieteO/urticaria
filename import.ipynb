{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import queue\n",
    "import time\n",
    "import tesserocr\n",
    "#from multiprocessing import cpu_count, Pool\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image import convert_from_bytes\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the source data\n",
    "The data is provided as a directory that is three levels deep (the third level is ommited in the following listing).\n",
    "``` bash\n",
    "fiete@ubu:~/Documents/studium/analyse_semi_und_unstrukturierter_daten$ tree -d -L 1 CAPTUM\n",
    "CAPTUM\n",
    "├── Allergic Diseases\n",
    "├── ANA\n",
    "├── Angioedema\n",
    "├── anti-FcεRI\n",
    "├── Antihistamine\n",
    "├── Anti-IgE\n",
    "├── anti-TPO IgE ratio\n",
    "├── ASST\n",
    "├── Basophil\n",
    "├── BAT\n",
    "├── BHRA\n",
    "├── CRP\n",
    "├── Cyclosporine\n",
    "├── D-Dimer\n",
    "├── dsDNA\n",
    "├── Duration\n",
    "├── Eosinophil\n",
    "├── IL-24\n",
    "├── Omalizumab\n",
    "├── Severity\n",
    "├── Thyroglobulin\n",
    "├── Total IgE\n",
    "└── TPO\n",
    "```\n",
    "\n",
    "To work further with the source data, it is useful to have a list of file paths for the pdfs. The following creates a list of all pdf files in the `CAPTUM` source folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(path):\n",
    "    pdf_filepaths = []\n",
    "    for root, directories, files in os.walk(path, topdown=False):\n",
    "        for name in files:\n",
    "            if name[-4:] == '.pdf':\n",
    "                pdf_filepaths.append(os.path.join(root, name))\n",
    "    return pdf_filepaths\n",
    "\n",
    "def get_language(path):\n",
    "        text = ''\n",
    "        for p in range(0,5):\n",
    "            image_path = path[:-4] + '_' + str(p) + '.jpg'\n",
    "            if os.path.isfile(image_path):\n",
    "                if len(text) <= 500:\n",
    "                    text += tesserocr.file_to_text(image_path)\n",
    "                else:\n",
    "                    print('Reached required number of words for language detection after ' + str(p) + ' pages.')\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        return detect(text[:500]) # returns i.e en or de\n",
    "\n",
    "tesserocr_queue = queue.Queue()\n",
    "\n",
    "def perform_ocr(img):\n",
    "    tess_api = None\n",
    "    try:\n",
    "        tess_api = tesserocr_queue.get(block=True, timeout=300)\n",
    "        tess_api.SetImage(img)\n",
    "        text = tess_api.GetUTF8Text()\n",
    "        return text\n",
    "    except tesserocr_queue.Empty:\n",
    "        print('Empty exception caught!')\n",
    "        return None\n",
    "    finally:\n",
    "        if tess_api is not None:\n",
    "            tesserocr_queue.put(tess_api)\n",
    "\n",
    "def run_threaded_ocr_on_pdf(ocr_images, num_threads, language):\n",
    "    # Setup Queue\n",
    "    for _ in range(num_threads):\n",
    "        tesserocr_queue.put(tesserocr.PyTessBaseAPI(lang=language))\n",
    "\n",
    "    # Perform OCR using ThreadPoolExecutor\n",
    "    start = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        res = executor.map(perform_ocr, ocr_images)\n",
    "    end = time.time()\n",
    "\n",
    "    # Restoring queue\n",
    "    for _ in range(num_threads):\n",
    "        api = tesserocr_queue.get(block=True)\n",
    "        api.End()\n",
    "\n",
    "    tesserocr_queue.queue.clear()\n",
    "    return (res, end - start)\n",
    "\n",
    "def ocr_pdf(filepath, language, threads):\n",
    "    # Pdf to image\n",
    "    with open(filepath, 'rb') as raw_pdf:\n",
    "        ocr_entities = convert_from_bytes(raw_pdf.read(), dpi=300, thread_count=4, grayscale=True)\n",
    "\n",
    "    print(f'Starting OCR for file { os.path.basename(filepath) }')\n",
    "    result_iterator, total_time = run_threaded_ocr_on_pdf(ocr_entities, threads, language)\n",
    "\n",
    "    text = ''\n",
    "    number_of_pages = 0\n",
    "    for item in result_iterator:\n",
    "        text += item\n",
    "        number_of_pages += 1\n",
    "    \n",
    "    print(f'OCR finished in {str(total_time)} seconds with an average of {str(total_time / number_of_pages)} seconds per page.')\n",
    "    return (text, number_of_pages)"
   ]
  },
  {
   "source": [
    "## Initialize dataframe with pdf filepaths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf\n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf\n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf\n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf\n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "root_dir = './CAPTUM'\n",
    "df = pd.DataFrame(get_filepaths(root_dir), columns = ['filepath'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for duplicate entries\n",
    "We can identify duplicate pdfs by computing the checksum of each file and then counting the unique values. So let us define the checksum function `get_checksum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
    "import hashlib\n",
    "\n",
    "def get_checksum(filepath: str) -> str:\n",
    "    # Open,close, read file and calculate MD5 on its contents \n",
    "    with open(filepath, 'rb') as file_to_check:\n",
    "        # read contents of the file\n",
    "        data = file_to_check.read()    \n",
    "        # pipe contents of the file through\n",
    "        return hashlib.md5(data).hexdigest()\n",
    "\n",
    "# check that it works\n",
    "file_one, file_one_copy, file_two = \"./pdf_1.pdf\", \"./pdf_1 copy.pdf\", \"./pdf_2.pdf\"\n",
    "assert get_checksum(file_one) == get_checksum(file_one_copy), \"should be equal\"\n",
    "assert get_checksum(file_one) != get_checksum(file_two), \"should not be equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a pandas dataframe from the list of filepath's and also add a checksum column that is computed using our `get_checksum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath                          checksum\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf  2fad223ae2232cb9e855d3ece9e34b72\n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf  c721aaea67a47811324b3c860dde612b\n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf  aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf  989e3eca08259c9a898acc551473f55f\n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf  2ed156f4fd5cfa00198f3f6f590940e0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df['checksum'] = df['filepath'].map(get_checksum)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we can analyse the results of this activity. It seems that our available data is in reality only half as large as it initially appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of pdfs: 1047\nTotal number of unique pdfs: 464\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       2fad223ae2232cb9e855d3ece9e34b72\n",
       "1       c721aaea67a47811324b3c860dde612b\n",
       "2       aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3       989e3eca08259c9a898acc551473f55f\n",
       "4       2ed156f4fd5cfa00198f3f6f590940e0\n",
       "                      ...               \n",
       "1042    fb22292adf8f35656fde0e54dc0cee51\n",
       "1043    6a5635468c99716fc18b91b7b6ebaeaf\n",
       "1044    6cfd7540663be0f6d7fb72f776339b71\n",
       "1045    849adffe6101df0a030cf425f661e1ed\n",
       "1046    f13be81ffbff55e031a34ef81d43cbff\n",
       "Name: checksum, Length: 1047, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "print('Total number of pdfs: {}'.format(df['checksum'].count()))\n",
    "print('Total number of unique pdfs: {}'.format(len(df['checksum'].unique())))\n",
    "df['checksum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a df of unique pdfs by removing duplicate checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath                          checksum\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf  2fad223ae2232cb9e855d3ece9e34b72\n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf  c721aaea67a47811324b3c860dde612b\n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf  aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf  989e3eca08259c9a898acc551473f55f\n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf  2ed156f4fd5cfa00198f3f6f590940e0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset=['checksum'])\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the text\n",
    "The next step is to read the text from the pdfs. We will do this using Optical Character Recognition (OCR)"
   ]
  },
  {
   "source": [
    "## Identify the document language based on a sample of pages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "Reached required number of words for language detection after 1 pages.\n",
      "<ipython-input-17-ad134c083fcd>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unique['lang'] = df_unique['filepath'].map(get_language)\n",
      "/home/fiete/.local/lib/python3.8/site-packages/pandas/core/generic.py:5494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            filepath  ... lang\n",
       "0                    ./CAPTUM/CRP/ANA/Asero 2017.pdf  ...  eng\n",
       "1  ./CAPTUM/Allergic Diseases/Omalizumab/Llanos 2...  ...  eng\n",
       "2  ./CAPTUM/Allergic Diseases/Omalizumab/Clark 20...  ...  eng\n",
       "3  ./CAPTUM/Allergic Diseases/Omalizumab/Damask 2...  ...  eng\n",
       "4  ./CAPTUM/Allergic Diseases/Omalizumab/Palacios...  ...  eng\n",
       "\n",
       "[5 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/Allergic Diseases/Omalizumab/Llanos 2...</td>\n      <td>eef921176bb03136228aefffb9727ae3</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/Allergic Diseases/Omalizumab/Clark 20...</td>\n      <td>c947cef954a9796cdc2bab0cd8507874</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/Allergic Diseases/Omalizumab/Damask 2...</td>\n      <td>3f5cbea57ace7d351641e634c7008478</td>\n      <td>eng</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/Allergic Diseases/Omalizumab/Palacios...</td>\n      <td>a39d7763465b87b81a72259037b3c158</td>\n      <td>eng</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df_unique['lang'] = df_unique['filepath'].map(get_language)\n",
    "df_unique.lang = df_unique.lang.map({'en':'eng','de':'deu'})\n",
    "df_unique = df_unique.sort_values(by='lang')\n",
    "df_unique.reset_index(drop=True, inplace=True)\n",
    "df_unique.head()"
   ]
  },
  {
   "source": [
    "### Running OCR on the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rage of 1.2425532937049866 seconds per page.\n",
      "Starting OCR for file Kyriakou 2018.pdf\n",
      "OCR finished in 2.7095561027526855 seconds with an average of 0.30106178919474286 seconds per page.\n",
      "Starting OCR for file Salman 2019  .pdf\n",
      "OCR finished in 7.834311008453369 seconds with an average of 0.3561050458387895 seconds per page.\n",
      "Starting OCR for file Maurer 2013.pdf\n",
      "OCR finished in 11.13097858428955 seconds with an average of 0.9275815486907959 seconds per page.\n",
      "Starting OCR for file Sanchez 2018.pdf\n",
      "OCR finished in 6.654712438583374 seconds with an average of 0.9506732055119106 seconds per page.\n",
      "Starting OCR for file Nam 2012.pdf\n",
      "OCR finished in 5.362809181213379 seconds with an average of 1.0725618362426759 seconds per page.\n",
      "Starting OCR for file Kulthanan 2017 .pdf\n",
      "OCR finished in 30.386348724365234 seconds with an average of 4.340906960623605 seconds per page.\n",
      "Starting OCR for file Asero 2018.pdf\n",
      "OCR finished in 2.859804630279541 seconds with an average of 0.408543518611363 seconds per page.\n",
      "Starting OCR for file Casale 2015.pdf\n",
      "OCR finished in 9.36781120300293 seconds with an average of 1.0408679114447699 seconds per page.\n",
      "Starting OCR for file Aghdam 2020.pdf\n",
      "OCR finished in 7.941840887069702 seconds with an average of 0.9927301108837128 seconds per page.\n",
      "Starting OCR for file Lapeere 2020.pdf\n",
      "OCR finished in 8.590224981307983 seconds with an average of 1.073778122663498 seconds per page.\n",
      "Starting OCR for file Ferrer 2017.pdf\n",
      "OCR finished in 9.070858240127563 seconds with an average of 1.0078731377919514 seconds per page.\n",
      "Starting OCR for file Vollono 2019.pdf\n",
      "OCR finished in 5.878398180007935 seconds with an average of 0.9797330300013224 seconds per page.\n",
      "Starting OCR for file Curto-Barredo 2018.pdf\n",
      "OCR finished in 3.120833158493042 seconds with an average of 1.0402777194976807 seconds per page.\n",
      "Starting OCR for file table final .pdf\n",
      "OCR finished in 3.02436900138855 seconds with an average of 0.3360410001542833 seconds per page.\n",
      "Starting OCR for file Asero 2011 .pdf\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Expected bytes, got float",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6c38be28aaa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocr_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'number_of_pages'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumber_of_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-9e1d6d8b8231>\u001b[0m in \u001b[0;36mocr_pdf\u001b[0;34m(filepath, language, threads)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Starting OCR for file { os.path.basename(filepath) }'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mresult_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_threaded_ocr_on_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mocr_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-9e1d6d8b8231>\u001b[0m in \u001b[0;36mrun_threaded_ocr_on_pdf\u001b[0;34m(ocr_images, num_threads, language)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Setup Queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtesserocr_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesserocr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTessBaseAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Perform OCR using ThreadPoolExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtesserocr.pyx\u001b[0m in \u001b[0;36mtesserocr.PyTessBaseAPI.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtesserocr.pyx\u001b[0m in \u001b[0;36mtesserocr._b\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected bytes, got float"
     ]
    }
   ],
   "source": [
    "# check optimal number of threads with tesser_perf.py\n",
    "threads = 8\n",
    "\n",
    "for index, row in df_unique.iterrows():\n",
    "    text, number_of_pages = ocr_pdf(row.filepath, row.lang ,threads)\n",
    "    df_unique.loc[index, 'text'] = text\n",
    "    df_unique.loc[index, 'number_of_pages'] = number_of_pages\n",
    "\n",
    "df_unique.number_of_pages = df_unique.number_of_pages.astype(int)   # is decimal otherwise\n",
    "print(df_unique.head())\n",
    "df_unique.to_csv('captum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}