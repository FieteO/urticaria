{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the source data\n",
    "The data is provided as a directory that is three levels deep (the third level is ommited in the following listing).\n",
    "``` bash\n",
    "fiete@ubu:~/Documents/studium/analyse_semi_und_unstrukturierter_daten$ tree -d -L 1 CAPTUM\n",
    "CAPTUM\n",
    "├── Allergic Diseases\n",
    "├── ANA\n",
    "├── Angioedema\n",
    "├── anti-FcεRI\n",
    "├── Antihistamine\n",
    "├── Anti-IgE\n",
    "├── anti-TPO IgE ratio\n",
    "├── ASST\n",
    "├── Basophil\n",
    "├── BAT\n",
    "├── BHRA\n",
    "├── CRP\n",
    "├── Cyclosporine\n",
    "├── D-Dimer\n",
    "├── dsDNA\n",
    "├── Duration\n",
    "├── Eosinophil\n",
    "├── IL-24\n",
    "├── Omalizumab\n",
    "├── Severity\n",
    "├── Thyroglobulin\n",
    "├── Total IgE\n",
    "└── TPO\n",
    "```\n",
    "\n",
    "To work further with the source data, it is useful to have a list of file paths for the pdfs. The following creates a list of all pdf files in the `CAPTUM` source folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./CAPTUM/CRP/ANA/Asero 2017.pdf',\n",
       " './CAPTUM/CRP/ANA/Magen 2015.pdf',\n",
       " './CAPTUM/CRP/Severity/Kolkhir 2017 .pdf',\n",
       " './CAPTUM/CRP/Severity/Baek 2014.pdf',\n",
       " './CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf']"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './CAPTUM'\n",
    "\n",
    "pdf_filepaths = []\n",
    "for root, directories, files in os.walk(path, topdown=False):\n",
    "\tfor name in files:\n",
    "\t\tif name[-4:] == '.pdf':\n",
    "\t\t\tpdf_filepaths.append(os.path.join(root, name))\n",
    "\n",
    "pdf_filepaths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for duplicate entries\n",
    "We can identify duplicate pdfs by computing the checksum of each file and then counting the unique values. So let us define the checksum function `get_checksum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
    "import hashlib\n",
    "\n",
    "def get_checksum(filepath: str) -> str:\n",
    "    # Open,close, read file and calculate MD5 on its contents \n",
    "    with open(filepath, 'rb') as file_to_check:\n",
    "        # read contents of the file\n",
    "        data = file_to_check.read()    \n",
    "        # pipe contents of the file through\n",
    "        return hashlib.md5(data).hexdigest()\n",
    "\n",
    "# check that it works\n",
    "file_one, file_one_copy, file_two = \"./pdf_1.pdf\", \"./pdf_1 copy.pdf\", \"./pdf_2.pdf\"\n",
    "assert get_checksum(file_one) == get_checksum(file_one_copy), \"should be equal\"\n",
    "assert get_checksum(file_one) != get_checksum(file_two), \"should not be equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a pandas dataframe from the list of filepath's and also add a checksum column that is computed using our `get_checksum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               filepath  \\\n",
       "0                       ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                       ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2               ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3                   ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4        ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "...                                                 ...   \n",
       "1042  ./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...   \n",
       "1043  ./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...   \n",
       "1044    ./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf   \n",
       "1045       ./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf   \n",
       "1046                          ./CAPTUM/table final .pdf   \n",
       "\n",
       "                              checksum  \n",
       "0     2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1     c721aaea67a47811324b3c860dde612b  \n",
       "2     aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3     989e3eca08259c9a898acc551473f55f  \n",
       "4     2ed156f4fd5cfa00198f3f6f590940e0  \n",
       "...                                ...  \n",
       "1042  fb22292adf8f35656fde0e54dc0cee51  \n",
       "1043  6a5635468c99716fc18b91b7b6ebaeaf  \n",
       "1044  6cfd7540663be0f6d7fb72f776339b71  \n",
       "1045  849adffe6101df0a030cf425f661e1ed  \n",
       "1046  f13be81ffbff55e031a34ef81d43cbff  \n",
       "\n",
       "[1047 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1042</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...</td>\n      <td>fb22292adf8f35656fde0e54dc0cee51</td>\n    </tr>\n    <tr>\n      <th>1043</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...</td>\n      <td>6a5635468c99716fc18b91b7b6ebaeaf</td>\n    </tr>\n    <tr>\n      <th>1044</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf</td>\n      <td>6cfd7540663be0f6d7fb72f776339b71</td>\n    </tr>\n    <tr>\n      <th>1045</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf</td>\n      <td>849adffe6101df0a030cf425f661e1ed</td>\n    </tr>\n    <tr>\n      <th>1046</th>\n      <td>./CAPTUM/table final .pdf</td>\n      <td>f13be81ffbff55e031a34ef81d43cbff</td>\n    </tr>\n  </tbody>\n</table>\n<p>1047 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pdf_filepaths, columns = ['filepath'])\n",
    "df['checksum'] = df['filepath'].apply(get_checksum)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we can analyse the results of this activity. It seems that our available data is in reality only half as large as it initially appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of pdfs: 1047\nTotal number of unique pdfs: 464\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       2fad223ae2232cb9e855d3ece9e34b72\n",
       "1       c721aaea67a47811324b3c860dde612b\n",
       "2       aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3       989e3eca08259c9a898acc551473f55f\n",
       "4       2ed156f4fd5cfa00198f3f6f590940e0\n",
       "                      ...               \n",
       "1042    fb22292adf8f35656fde0e54dc0cee51\n",
       "1043    6a5635468c99716fc18b91b7b6ebaeaf\n",
       "1044    6cfd7540663be0f6d7fb72f776339b71\n",
       "1045    849adffe6101df0a030cf425f661e1ed\n",
       "1046    f13be81ffbff55e031a34ef81d43cbff\n",
       "Name: checksum, Length: 1047, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "print('Total number of pdfs: {}'.format(df['checksum'].count()))\n",
    "print('Total number of unique pdfs: {}'.format(len(df['checksum'].unique())))\n",
    "df['checksum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a df of unique pdfs by removing duplicate checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath  \\\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "\n",
       "                           checksum  \n",
       "0  2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1  c721aaea67a47811324b3c860dde612b  \n",
       "2  aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3  989e3eca08259c9a898acc551473f55f  \n",
       "4  2ed156f4fd5cfa00198f3f6f590940e0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset=['checksum'])\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the text\n",
    "The next step is to read the text from the pdfs. We can try two different approaches to this problem:\n",
    "- [Using pdfminer.six][#using-pdfminersix]\n",
    "- [Using Optical Character Recognition (OCR)](#using-optical-character-recognition-ocr)"
   ]
  },
  {
   "source": [
    "## Using pdfminer.six\n",
    "First we import the neccessary modules (more on pdfminer.six [here](https://pdfminersix.readthedocs.io/en/latest/)) and create a procedure to extract the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "\toutput_string = StringIO()\n",
    "\twith open(file_path, 'rb') as in_file:\n",
    "\t    parser = PDFParser(in_file)\n",
    "\t    doc = PDFDocument(parser)\n",
    "\t    rsrcmgr = PDFResourceManager()\n",
    "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\t    for page in PDFPage.create_pages(doc):\n",
    "\t        interpreter.process_page(page)\n",
    "\n",
    "\treturn(output_string.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over each individual file and use pdfminer to get the file content. You can run either one of the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "    # write the content to a new column of the dataframe and save as csv\n",
    "    # this takes between five and ten minutes\n",
    "    df_unique['text'] = df_unique.filepath.apply(lambda fp: convert_pdf_to_string(fp))\n",
    "    df_unique.to_csv('captum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Output Path if it exists, then create new\n",
    "import shutil\n",
    "out_dir = \"./out/\"\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "# Alternative approach to cell above,\n",
    "# replace with True if this should be run\n",
    "if (False):\n",
    "    # Iterate over each individual file\n",
    "    for index, row in df_unique.iterrows():\n",
    "        path = row['filepath']\n",
    "        checksum = row['checksum']\n",
    "\n",
    "        content = convert_pdf_to_string(path)\n",
    "        \n",
    "        out_file_path = os.path.join(out_dir, checksum + \".txt\")\n",
    "        \n",
    "        with open(out_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(content)\n",
    "            \n",
    "        if ((index + 1) % 10 == 0):\n",
    "            print(str(index + 1) + \"/\" + str(len(df_unique)) + \" done\")"
   ]
  },
  {
   "source": [
    "## Using Optical Character Recognition (OCR)\n",
    "Also we can try to use Optical Character Recognition (OCR) on the pdfs to get a better result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-60-c0862fcc6aef>:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_unique['number_of_pages'] = df_unique['filepath'].apply(lambda fp: save_as_image(fp))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath  \\\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "\n",
       "                           checksum  number_of_pages  \n",
       "0  2fad223ae2232cb9e855d3ece9e34b72                5  \n",
       "1  c721aaea67a47811324b3c860dde612b                7  \n",
       "2  aed2cb292fdffefe2a319b9d7e517bb3                4  \n",
       "3  989e3eca08259c9a898acc551473f55f                6  \n",
       "4  2ed156f4fd5cfa00198f3f6f590940e0                9  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n      <th>number_of_pages</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052\n",
    "# https://pdf2image.readthedocs.io/en/latest/reference.html\n",
    "# this is also getting the page number because of performance reasons\n",
    "def save_as_image(filepath):\n",
    "    pages = convert_from_path(filepath)\n",
    "    for p in range(len(pages)):\n",
    "        pages[p].save(filepath[:-4] + '_' + str(p) + '.jpg', 'JPEG')\n",
    "    return len(pages)\n",
    "\n",
    "# df_unique = df_unique[:10]\n",
    "if (True):\n",
    "    df_unique['number_of_pages'] = df_unique['filepath'].apply(lambda fp: save_as_image(fp))\n",
    "#df_unique['number_of_pages'] = df_unique.apply(lambda row: save_as_image(row['filepath']), axis=1)\n",
    "\n",
    "#range(df_unique['number_of_pages'])\n",
    "#print(pytesseract.image_to_string(Image.open(df_unique['filepath'][0][:-4] + '_0.jpg')))\n",
    "\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytesseract import image_to_string\n",
    "\n",
    "def text_from_ocr(filepath, number_of_pages):\n",
    "    text = ''\n",
    "    for page in range(number_of_pages):\n",
    "        text += image_to_string(filepath[:-4] + '_' + str(page) + '.jpg')\n",
    "    return text\n",
    "\n",
    "if (False):\n",
    "    df_unique['text'] = df_unique.apply(lambda row: text_from_ocr(row['filepath'], row['number_of_pages']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of cores available: 12\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'filepath' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-65-20f8eb15a000>\", line 18, in text_from_ocr\n    text += image_to_string(filepath[:-4] + '_' + str(page) + '.jpg')\nNameError: name 'filepath' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-20f8eb15a000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of cores available: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched_getaffinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdf_unique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallelize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_from_ocr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched_getaffinity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-20f8eb15a000>\u001b[0m in \u001b[0;36mparallelize_dataframe\u001b[0;34m(df, func, n_cores)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdf_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filepath' is not defined"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "import numpy as np\n",
    "\n",
    "def text_from_ocr(df) -> pd.DataFrame:\n",
    "    for index, row in df.iterrows():\n",
    "        text = ''\n",
    "        for page in range(row['number_of_pages']):\n",
    "            text += image_to_string(row['filepath'][:-4] + '_' + str(page) + '.jpg')\n",
    "        df.loc[index, 'text'] = text\n",
    "    return df\n",
    "\n",
    "# https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1#6028\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# This will take more than half an hour\n",
    "df_unique = parallelize_dataframe(df_unique, text_from_ocr, len(os.sched_getaffinity(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.to_csv('captum.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}