{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the source data\n",
    "The data is provided as a directory that is three levels deep (the third level is ommited in the following listing).\n",
    "``` bash\n",
    "fiete@ubu:~/Documents/studium/analyse_semi_und_unstrukturierter_daten$ tree -d -L 1 CAPTUM\n",
    "CAPTUM\n",
    "├── Allergic Diseases\n",
    "├── ANA\n",
    "├── Angioedema\n",
    "├── anti-FcεRI\n",
    "├── Antihistamine\n",
    "├── Anti-IgE\n",
    "├── anti-TPO IgE ratio\n",
    "├── ASST\n",
    "├── Basophil\n",
    "├── BAT\n",
    "├── BHRA\n",
    "├── CRP\n",
    "├── Cyclosporine\n",
    "├── D-Dimer\n",
    "├── dsDNA\n",
    "├── Duration\n",
    "├── Eosinophil\n",
    "├── IL-24\n",
    "├── Omalizumab\n",
    "├── Severity\n",
    "├── Thyroglobulin\n",
    "├── Total IgE\n",
    "└── TPO\n",
    "```\n",
    "\n",
    "To work further with the source data, it is useful to have a list of file paths for the pdfs. The following creates a list of all pdf files in the `CAPTUM` source folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./CAPTUM/CRP/ANA/Asero 2017.pdf',\n",
       " './CAPTUM/CRP/ANA/Magen 2015.pdf',\n",
       " './CAPTUM/CRP/Severity/Kolkhir 2017 .pdf',\n",
       " './CAPTUM/CRP/Severity/Baek 2014.pdf',\n",
       " './CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './CAPTUM'\n",
    "\n",
    "pdf_filepaths = []\n",
    "for root, directories, files in os.walk(path, topdown=False):\n",
    "\tfor name in files:\n",
    "\t\tif name[-4:] == '.pdf':\n",
    "\t\t\tpdf_filepaths.append(os.path.join(root, name))\n",
    "\n",
    "pdf_filepaths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for duplicate entries\n",
    "We can identify duplicate pdfs by computing the checksum of each file and then counting the unique values. So let us define the checksum function `get_checksum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
    "import hashlib\n",
    "\n",
    "def get_checksum(filepath: str) -> str:\n",
    "    # Open,close, read file and calculate MD5 on its contents \n",
    "    with open(filepath, 'rb') as file_to_check:\n",
    "        # read contents of the file\n",
    "        data = file_to_check.read()    \n",
    "        # pipe contents of the file through\n",
    "        return hashlib.md5(data).hexdigest()\n",
    "\n",
    "# check that it works\n",
    "file_one, file_one_copy, file_two = \"./pdf_1.pdf\", \"./pdf_1 copy.pdf\", \"./pdf_2.pdf\"\n",
    "assert get_checksum(file_one) == get_checksum(file_one_copy), \"should be equal\"\n",
    "assert get_checksum(file_one) != get_checksum(file_two), \"should not be equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a pandas dataframe from the list of filepath's and also add a checksum column that is computed using our `get_checksum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               filepath  \\\n",
       "0                       ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                       ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2               ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3                   ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4        ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "...                                                 ...   \n",
       "1042  ./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...   \n",
       "1043  ./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...   \n",
       "1044    ./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf   \n",
       "1045       ./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf   \n",
       "1046                          ./CAPTUM/table final .pdf   \n",
       "\n",
       "                              checksum  \n",
       "0     2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1     c721aaea67a47811324b3c860dde612b  \n",
       "2     aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3     989e3eca08259c9a898acc551473f55f  \n",
       "4     2ed156f4fd5cfa00198f3f6f590940e0  \n",
       "...                                ...  \n",
       "1042  fb22292adf8f35656fde0e54dc0cee51  \n",
       "1043  6a5635468c99716fc18b91b7b6ebaeaf  \n",
       "1044  6cfd7540663be0f6d7fb72f776339b71  \n",
       "1045  849adffe6101df0a030cf425f661e1ed  \n",
       "1046  f13be81ffbff55e031a34ef81d43cbff  \n",
       "\n",
       "[1047 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1042</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...</td>\n      <td>fb22292adf8f35656fde0e54dc0cee51</td>\n    </tr>\n    <tr>\n      <th>1043</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...</td>\n      <td>6a5635468c99716fc18b91b7b6ebaeaf</td>\n    </tr>\n    <tr>\n      <th>1044</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf</td>\n      <td>6cfd7540663be0f6d7fb72f776339b71</td>\n    </tr>\n    <tr>\n      <th>1045</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf</td>\n      <td>849adffe6101df0a030cf425f661e1ed</td>\n    </tr>\n    <tr>\n      <th>1046</th>\n      <td>./CAPTUM/table final .pdf</td>\n      <td>f13be81ffbff55e031a34ef81d43cbff</td>\n    </tr>\n  </tbody>\n</table>\n<p>1047 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pdf_filepaths, columns = ['filepath'])\n",
    "df['checksum'] = df['filepath'].apply(get_checksum)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we can analyse the results of this activity. It seems that our available data is in reality only half as large as it initially appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of pdfs: 1047\nTotal number of unique pdfs: 464\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       2fad223ae2232cb9e855d3ece9e34b72\n",
       "1       c721aaea67a47811324b3c860dde612b\n",
       "2       aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3       989e3eca08259c9a898acc551473f55f\n",
       "4       2ed156f4fd5cfa00198f3f6f590940e0\n",
       "                      ...               \n",
       "1042    fb22292adf8f35656fde0e54dc0cee51\n",
       "1043    6a5635468c99716fc18b91b7b6ebaeaf\n",
       "1044    6cfd7540663be0f6d7fb72f776339b71\n",
       "1045    849adffe6101df0a030cf425f661e1ed\n",
       "1046    f13be81ffbff55e031a34ef81d43cbff\n",
       "Name: checksum, Length: 1047, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "print('Total number of pdfs: {}'.format(df['checksum'].count()))\n",
    "print('Total number of unique pdfs: {}'.format(len(df['checksum'].unique())))\n",
    "df['checksum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a df of unique pdfs by removing duplicate checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         filepath  \\\n",
       "0                 ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                 ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2         ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3             ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4  ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "\n",
       "                           checksum  \n",
       "0  2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1  c721aaea67a47811324b3c860dde612b  \n",
       "2  aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3  989e3eca08259c9a898acc551473f55f  \n",
       "4  2ed156f4fd5cfa00198f3f6f590940e0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset=['checksum'])\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the text\n",
    "The next step is to read the text from the pdfs. We can try two different approaches to this problem:\n",
    "- [Using pdfminer.six][#using-pdfminersix]\n",
    "- [Using Optical Character Recognition (OCR)](#using-optical-character-recognition-ocr)"
   ]
  },
  {
   "source": [
    "## Using pdfminer.six\n",
    "First we import the neccessary modules (more on pdfminer.six [here](https://pdfminersix.readthedocs.io/en/latest/)) and create a procedure to extract the text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "\toutput_string = StringIO()\n",
    "\twith open(file_path, 'rb') as in_file:\n",
    "\t    parser = PDFParser(in_file)\n",
    "\t    doc = PDFDocument(parser)\n",
    "\t    rsrcmgr = PDFResourceManager()\n",
    "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\t    for page in PDFPage.create_pages(doc):\n",
    "\t        interpreter.process_page(page)\n",
    "\n",
    "\treturn(output_string.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over each individual file and use pdfminer to get the file content. You can run either one of the two cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "    # write the content to a new column of the dataframe and save as csv\n",
    "    # this takes between five and ten minutes\n",
    "    df_unique['text'] = df_unique.filepath.apply(lambda fp: convert_pdf_to_string(fp))\n",
    "    df_unique.to_csv('captum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove Output Path if it exists, then create new\n",
    "import shutil\n",
    "out_dir = \"./out/\"\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "# Alternative approach to cell above,\n",
    "# replace with True if this should be run\n",
    "if (False):\n",
    "    # Iterate over each individual file\n",
    "    for index, row in df_unique.iterrows():\n",
    "        path = row['filepath']\n",
    "        checksum = row['checksum']\n",
    "\n",
    "        content = convert_pdf_to_string(path)\n",
    "        \n",
    "        out_file_path = os.path.join(out_dir, checksum + \".txt\")\n",
    "        \n",
    "        with open(out_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(content)\n",
    "            \n",
    "        if ((index + 1) % 10 == 0):\n",
    "            print(str(index + 1) + \"/\" + str(len(df_unique)) + \" done\")"
   ]
  },
  {
   "source": [
    "## Using Optical Character Recognition (OCR)\n",
    "Also we can try to use Optical Character Recognition (OCR) on the pdfs to get a better result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Saving the pdfs as images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-6428b72d4b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number_of_pages'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filepath'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msave_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-6428b72d4b51>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fp)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'number_of_pages'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filepath'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msave_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf_unique\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-6428b72d4b51>\u001b[0m in \u001b[0;36msave_as_image\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# this is also getting the page number because of performance reasons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_as_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   1864\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1866\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1867\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# https://towardsdatascience.com/extracting-text-from-scanned-pdf-using-pytesseract-open-cv-cd670ee38052\n",
    "# https://pdf2image.readthedocs.io/en/latest/reference.html\n",
    "# this is also getting the page number because of performance reasons\n",
    "def save_as_image(filepath):\n",
    "    pages = convert_from_path(filepath)\n",
    "    for p in range(len(pages)):\n",
    "        path = filepath[:-4] + '_' + str(p) + '.jpg'\n",
    "        # only save if file does not exist\n",
    "        if (os.path.isfile(path) == False):\n",
    "            pages[p].save(path, 'JPEG')\n",
    "    return len(pages)\n",
    "\n",
    "if (True):\n",
    "    df_unique['number_of_pages'] = df_unique['filepath'].apply(lambda fp: save_as_image(fp))\n",
    "\n",
    "df_unique.to_csv('captum.csv')\n",
    "df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytesseract import image_to_string\n",
    "\n",
    "def text_from_ocr(filepath, number_of_pages):\n",
    "    text = ''\n",
    "    for page in range(number_of_pages):\n",
    "        text += image_to_string(filepath[:-4] + '_' + str(page) + '.jpg')\n",
    "    return text\n",
    "\n",
    "if (False):\n",
    "    df_unique['text'] = df_unique.apply(lambda row: text_from_ocr(row['filepath'], row['number_of_pages']), axis=1)"
   ]
  },
  {
   "source": [
    "### Running OCR on the images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "import numpy as np\n",
    "\n",
    "def text_from_ocr(df) -> pd.DataFrame:\n",
    "    df.head()\n",
    "    for index, row in df.iterrows():\n",
    "        print('entered row' + index)\n",
    "        text = ''\n",
    "        for page in range(row['number_of_pages']):\n",
    "            print('# of pages' + row['number_of_pages'])\n",
    "            text += image_to_string(row['filepath'][:-4] + '_' + str(page) + '.jpg')\n",
    "            print('text: ' + text)\n",
    "        df.loc[index, 'text'] = text\n",
    "        print(index)\n",
    "    return df\n",
    "\n",
    "# https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1#6028\n",
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "df_unique = pd.read_csv('captum.csv')\n",
    "\n",
    "# This will take more than two and a half hours\n",
    "df_unique = parallelize_dataframe(df_unique, text_from_ocr, len(os.sched_getaffinity(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.to_csv('captum.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}