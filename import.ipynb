{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the source data\n",
    "The data is provided as a directory that is three levels deep (the third level is ommited in the following listing).\n",
    "``` bash\n",
    "fiete@ubu:~/Documents/studium/analyse_semi_und_unstrukturierter_daten$ tree -d -L 1 CAPTUM\n",
    "CAPTUM\n",
    "├── Allergic Diseases\n",
    "├── ANA\n",
    "├── Angioedema\n",
    "├── anti-FcεRI\n",
    "├── Antihistamine\n",
    "├── Anti-IgE\n",
    "├── anti-TPO IgE ratio\n",
    "├── ASST\n",
    "├── Basophil\n",
    "├── BAT\n",
    "├── BHRA\n",
    "├── CRP\n",
    "├── Cyclosporine\n",
    "├── D-Dimer\n",
    "├── dsDNA\n",
    "├── Duration\n",
    "├── Eosinophil\n",
    "├── IL-24\n",
    "├── Omalizumab\n",
    "├── Severity\n",
    "├── Thyroglobulin\n",
    "├── Total IgE\n",
    "└── TPO\n",
    "```\n",
    "\n",
    "To work further with the source data, it is useful to have a list of file paths for the pdfs. The following creates a list of all pdf files in the `CAPTUM` source folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./CAPTUM\\\\Allergic Diseases\\\\Angioedema\\\\Arik yilmaz 2017.pdf',\n",
       " './CAPTUM\\\\Allergic Diseases\\\\Angioedema\\\\Bruno 2001.pdf',\n",
       " './CAPTUM\\\\Allergic Diseases\\\\Angioedema\\\\Cousin 2016.pdf',\n",
       " './CAPTUM\\\\Allergic Diseases\\\\Angioedema\\\\Faisant 2016.pdf',\n",
       " './CAPTUM\\\\Allergic Diseases\\\\Angioedema\\\\Kahveci 2020.pdf']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './CAPTUM'\n",
    "\n",
    "pdf_filepaths = []\n",
    "for root, directories, files in os.walk(path, topdown=False):\n",
    "\tfor name in files:\n",
    "\t\tpdf_filepaths.append(os.path.join(root, name))\n",
    "\n",
    "pdf_filepaths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for duplicate entries\n",
    "We can identify duplicate pdfs by computing the checksum of each file and then counting the unique values. So let us define the checksum function `get_checksum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
    "import hashlib\n",
    "\n",
    "def get_checksum(filepath: str) -> str:\n",
    "    # Open,close, read file and calculate MD5 on its contents \n",
    "    with open(filepath, 'rb') as file_to_check:\n",
    "        # read contents of the file\n",
    "        data = file_to_check.read()    \n",
    "        # pipe contents of the file through\n",
    "        return hashlib.md5(data).hexdigest()\n",
    "\n",
    "# check that it works\n",
    "file_one, file_two, file_three = \"./Arik yilmaz 2017.pdf\", \"./Arik yilmaz 2017 (copy).pdf\", \"./Bruno 2001.pdf\"\n",
    "assert get_checksum(file_one) == get_checksum(file_two), \"should be equal\"\n",
    "assert get_checksum(file_one) != get_checksum(file_three), \"should not be equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a pandas dataframe from the list of filepath's and also add a checksum column that is computed using our `get_checksum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>checksum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./CAPTUM\\Allergic Diseases\\Angioedema\\Arik yil...</td>\n",
       "      <td>ad656fbed80a09bc5a842a528cbcfa5d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./CAPTUM\\Allergic Diseases\\Angioedema\\Bruno 20...</td>\n",
       "      <td>6e0337369eae48049f7f080b48ca3af9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./CAPTUM\\Allergic Diseases\\Angioedema\\Cousin 2...</td>\n",
       "      <td>b53f40ffe6c949eb06d7098d66e10fca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./CAPTUM\\Allergic Diseases\\Angioedema\\Faisant ...</td>\n",
       "      <td>2882866de4e0ad21634941674bd81fe4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./CAPTUM\\Allergic Diseases\\Angioedema\\Kahveci ...</td>\n",
       "      <td>6f1b0a59e73bedae5a5250aa82500c26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>./CAPTUM\\TPO\\Thyroglobulin\\Sanchez 2020.pdf</td>\n",
       "      <td>7374451f8e1a341658d500b5577074e0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>./CAPTUM\\TPO\\Thyroglobulin\\Silvares 2017.pdf</td>\n",
       "      <td>c85a07aeb5807151c5479d1c2c219a0f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>./CAPTUM\\TPO\\Thyroglobulin\\Wan 2012.pdf</td>\n",
       "      <td>c4f1d27a5e3bcc7f023c0d180376e0d7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>./CAPTUM\\desktop.ini</td>\n",
       "      <td>15478b340a8362bb79fd2a6ea0dde1a0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>./CAPTUM\\table final .pdf</td>\n",
       "      <td>f13be81ffbff55e031a34ef81d43cbff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1060 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               filepath  \\\n",
       "0     ./CAPTUM\\Allergic Diseases\\Angioedema\\Arik yil...   \n",
       "1     ./CAPTUM\\Allergic Diseases\\Angioedema\\Bruno 20...   \n",
       "2     ./CAPTUM\\Allergic Diseases\\Angioedema\\Cousin 2...   \n",
       "3     ./CAPTUM\\Allergic Diseases\\Angioedema\\Faisant ...   \n",
       "4     ./CAPTUM\\Allergic Diseases\\Angioedema\\Kahveci ...   \n",
       "...                                                 ...   \n",
       "1055        ./CAPTUM\\TPO\\Thyroglobulin\\Sanchez 2020.pdf   \n",
       "1056       ./CAPTUM\\TPO\\Thyroglobulin\\Silvares 2017.pdf   \n",
       "1057            ./CAPTUM\\TPO\\Thyroglobulin\\Wan 2012.pdf   \n",
       "1058                               ./CAPTUM\\desktop.ini   \n",
       "1059                          ./CAPTUM\\table final .pdf   \n",
       "\n",
       "                              checksum  \n",
       "0     ad656fbed80a09bc5a842a528cbcfa5d  \n",
       "1     6e0337369eae48049f7f080b48ca3af9  \n",
       "2     b53f40ffe6c949eb06d7098d66e10fca  \n",
       "3     2882866de4e0ad21634941674bd81fe4  \n",
       "4     6f1b0a59e73bedae5a5250aa82500c26  \n",
       "...                                ...  \n",
       "1055  7374451f8e1a341658d500b5577074e0  \n",
       "1056  c85a07aeb5807151c5479d1c2c219a0f  \n",
       "1057  c4f1d27a5e3bcc7f023c0d180376e0d7  \n",
       "1058  15478b340a8362bb79fd2a6ea0dde1a0  \n",
       "1059  f13be81ffbff55e031a34ef81d43cbff  \n",
       "\n",
       "[1060 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pdf_filepaths, columns = ['filepath'])\n",
    "df['checksum'] = df['filepath'].apply(get_checksum)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we can analyse the results of this activity. It seems that our available data is in reality only half as large as it initially appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pdfs: 1060\n",
      "Total number of unique pdfs: 467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       ad656fbed80a09bc5a842a528cbcfa5d\n",
       "1       6e0337369eae48049f7f080b48ca3af9\n",
       "2       b53f40ffe6c949eb06d7098d66e10fca\n",
       "3       2882866de4e0ad21634941674bd81fe4\n",
       "4       6f1b0a59e73bedae5a5250aa82500c26\n",
       "                      ...               \n",
       "1055    7374451f8e1a341658d500b5577074e0\n",
       "1056    c85a07aeb5807151c5479d1c2c219a0f\n",
       "1057    c4f1d27a5e3bcc7f023c0d180376e0d7\n",
       "1058    15478b340a8362bb79fd2a6ea0dde1a0\n",
       "1059    f13be81ffbff55e031a34ef81d43cbff\n",
       "Name: checksum, Length: 1060, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total number of pdfs: {}'.format(df['checksum'].count()))\n",
    "print('Total number of unique pdfs: {}'.format(len(df['checksum'].unique())))\n",
    "df['checksum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a df of unique pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "df_unique = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read the text from the pdfs\n",
    "We use aws textract. To install the boto3 client you need to add boto3 to your environment\n",
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Todo: Iterate over paths\n",
    "path = df_unique.iloc[0]['filepath']\n",
    "\n",
    "# Read document content\n",
    "with open(path, 'rb') as pdf:\n",
    "    imageBytes = bytearray(pdf.read())\n",
    "    \n",
    "# Amazon Textract client\n",
    "textract = boto3.client(    \n",
    "    'textract', \n",
    "    region_name='<region>', \n",
    "    aws_access_key_id='<key_id>', \n",
    "    aws_secret_access_key='<key>'\n",
    ")\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.detect_document_text(Document={'Bytes': imageBytes})\n",
    "\n",
    "#print(response)\n",
    "\n",
    "# Print detected text\n",
    "for item in response[\"Blocks\"]:\n",
    "    if item[\"BlockType\"] == \"LINE\":\n",
    "        print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')\n",
    "# TODO:\n",
    "# Find different ways to interact with the result. Read tables. Read Abstract, Contributors, Text, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
