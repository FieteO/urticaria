{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the source data\n",
    "The data is provided as a directory that is three levels deep (the third level is ommited in the following listing).\n",
    "``` bash\n",
    "fiete@ubu:~/Documents/studium/analyse_semi_und_unstrukturierter_daten$ tree -d -L 1 CAPTUM\n",
    "CAPTUM\n",
    "├── Allergic Diseases\n",
    "├── ANA\n",
    "├── Angioedema\n",
    "├── anti-FcεRI\n",
    "├── Antihistamine\n",
    "├── Anti-IgE\n",
    "├── anti-TPO IgE ratio\n",
    "├── ASST\n",
    "├── Basophil\n",
    "├── BAT\n",
    "├── BHRA\n",
    "├── CRP\n",
    "├── Cyclosporine\n",
    "├── D-Dimer\n",
    "├── dsDNA\n",
    "├── Duration\n",
    "├── Eosinophil\n",
    "├── IL-24\n",
    "├── Omalizumab\n",
    "├── Severity\n",
    "├── Thyroglobulin\n",
    "├── Total IgE\n",
    "└── TPO\n",
    "```\n",
    "\n",
    "To work further with the source data, it is useful to have a list of file paths for the pdfs. The following creates a list of all pdf files in the `CAPTUM` source folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./CAPTUM/CRP/ANA/Asero 2017.pdf',\n",
       " './CAPTUM/CRP/ANA/Magen 2015.pdf',\n",
       " './CAPTUM/CRP/Severity/Kolkhir 2017 .pdf',\n",
       " './CAPTUM/CRP/Severity/Baek 2014.pdf',\n",
       " './CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = './CAPTUM'\n",
    "\n",
    "pdf_filepaths = []\n",
    "for root, directories, files in os.walk(path, topdown=False):\n",
    "\tfor name in files:\n",
    "\t\tpdf_filepaths.append(os.path.join(root, name))\n",
    "\n",
    "pdf_filepaths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for duplicate entries\n",
    "We can identify duplicate pdfs by computing the checksum of each file and then counting the unique values. So let us define the checksum function `get_checksum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python#16876405\n",
    "import hashlib\n",
    "\n",
    "def get_checksum(filepath: str) -> str:\n",
    "    # Open,close, read file and calculate MD5 on its contents \n",
    "    with open(filepath, 'rb') as file_to_check:\n",
    "        # read contents of the file\n",
    "        data = file_to_check.read()    \n",
    "        # pipe contents of the file through\n",
    "        return hashlib.md5(data).hexdigest()\n",
    "\n",
    "# check that it works\n",
    "file_one, file_one_copy, file_two = \"./pdf_1.pdf\", \"./pdf_1 copy.pdf\", \"./pdf_2.pdf\"\n",
    "assert get_checksum(file_one) == get_checksum(file_one_copy), \"should be equal\"\n",
    "assert get_checksum(file_one) != get_checksum(file_two), \"should not be equal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a pandas dataframe from the list of filepath's and also add a checksum column that is computed using our `get_checksum()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               filepath  \\\n",
       "0                       ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                       ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2               ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3                   ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4        ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "...                                                 ...   \n",
       "1054  ./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...   \n",
       "1055  ./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...   \n",
       "1056    ./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf   \n",
       "1057       ./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf   \n",
       "1058                          ./CAPTUM/table final .pdf   \n",
       "\n",
       "                              checksum  \n",
       "0     2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1     c721aaea67a47811324b3c860dde612b  \n",
       "2     aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3     989e3eca08259c9a898acc551473f55f  \n",
       "4     2ed156f4fd5cfa00198f3f6f590940e0  \n",
       "...                                ...  \n",
       "1054  fb22292adf8f35656fde0e54dc0cee51  \n",
       "1055  6a5635468c99716fc18b91b7b6ebaeaf  \n",
       "1056  6cfd7540663be0f6d7fb72f776339b71  \n",
       "1057  849adffe6101df0a030cf425f661e1ed  \n",
       "1058  f13be81ffbff55e031a34ef81d43cbff  \n",
       "\n",
       "[1059 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...</td>\n      <td>fb22292adf8f35656fde0e54dc0cee51</td>\n    </tr>\n    <tr>\n      <th>1055</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Gimenez Arnau...</td>\n      <td>6a5635468c99716fc18b91b7b6ebaeaf</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf</td>\n      <td>6cfd7540663be0f6d7fb72f776339b71</td>\n    </tr>\n    <tr>\n      <th>1057</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Ke 2017.pdf</td>\n      <td>849adffe6101df0a030cf425f661e1ed</td>\n    </tr>\n    <tr>\n      <th>1058</th>\n      <td>./CAPTUM/table final .pdf</td>\n      <td>f13be81ffbff55e031a34ef81d43cbff</td>\n    </tr>\n  </tbody>\n</table>\n<p>1059 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pdf_filepaths, columns = ['filepath'])\n",
    "df['checksum'] = df['filepath'].apply(get_checksum)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final step, we can analyse the results of this activity. It seems that our available data is in reality only half as large as it initially appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of pdfs: 1059\nTotal number of unique pdfs: 466\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       2fad223ae2232cb9e855d3ece9e34b72\n",
       "1       c721aaea67a47811324b3c860dde612b\n",
       "2       aed2cb292fdffefe2a319b9d7e517bb3\n",
       "3       989e3eca08259c9a898acc551473f55f\n",
       "4       2ed156f4fd5cfa00198f3f6f590940e0\n",
       "                      ...               \n",
       "1054    fb22292adf8f35656fde0e54dc0cee51\n",
       "1055    6a5635468c99716fc18b91b7b6ebaeaf\n",
       "1056    6cfd7540663be0f6d7fb72f776339b71\n",
       "1057    849adffe6101df0a030cf425f661e1ed\n",
       "1058    f13be81ffbff55e031a34ef81d43cbff\n",
       "Name: checksum, Length: 1059, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "print('Total number of pdfs: {}'.format(df['checksum'].count()))\n",
    "print('Total number of unique pdfs: {}'.format(len(df['checksum'].unique())))\n",
    "df['checksum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a df of unique pdfs by removing duplicate checksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               filepath  \\\n",
       "0                       ./CAPTUM/CRP/ANA/Asero 2017.pdf   \n",
       "1                       ./CAPTUM/CRP/ANA/Magen 2015.pdf   \n",
       "2               ./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf   \n",
       "3                   ./CAPTUM/CRP/Severity/Baek 2014.pdf   \n",
       "4        ./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf   \n",
       "...                                                 ...   \n",
       "1050  ./CAPTUM/Omalizumab/Cyclosporine/Sánchez 2020.pdf   \n",
       "1053     ./CAPTUM/Omalizumab/Cyclosporine/Seth 2016.pdf   \n",
       "1054  ./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...   \n",
       "1056    ./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf   \n",
       "1058                          ./CAPTUM/table final .pdf   \n",
       "\n",
       "                              checksum  \n",
       "0     2fad223ae2232cb9e855d3ece9e34b72  \n",
       "1     c721aaea67a47811324b3c860dde612b  \n",
       "2     aed2cb292fdffefe2a319b9d7e517bb3  \n",
       "3     989e3eca08259c9a898acc551473f55f  \n",
       "4     2ed156f4fd5cfa00198f3f6f590940e0  \n",
       "...                                ...  \n",
       "1050  167014bf6002af9c8c62794730d3473e  \n",
       "1053  0bd4ab01b7e3f79ea404da26da3834b1  \n",
       "1054  fb22292adf8f35656fde0e54dc0cee51  \n",
       "1056  6cfd7540663be0f6d7fb72f776339b71  \n",
       "1058  f13be81ffbff55e031a34ef81d43cbff  \n",
       "\n",
       "[466 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>checksum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./CAPTUM/CRP/ANA/Asero 2017.pdf</td>\n      <td>2fad223ae2232cb9e855d3ece9e34b72</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./CAPTUM/CRP/ANA/Magen 2015.pdf</td>\n      <td>c721aaea67a47811324b3c860dde612b</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./CAPTUM/CRP/Severity/Kolkhir 2017 .pdf</td>\n      <td>aed2cb292fdffefe2a319b9d7e517bb3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./CAPTUM/CRP/Severity/Baek 2014.pdf</td>\n      <td>989e3eca08259c9a898acc551473f55f</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./CAPTUM/CRP/Severity/Kasperska-Zajac 2015.pdf</td>\n      <td>2ed156f4fd5cfa00198f3f6f590940e0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1050</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Sánchez 2020.pdf</td>\n      <td>167014bf6002af9c8c62794730d3473e</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Seth 2016.pdf</td>\n      <td>0bd4ab01b7e3f79ea404da26da3834b1</td>\n    </tr>\n    <tr>\n      <th>1054</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Rosenblum 202...</td>\n      <td>fb22292adf8f35656fde0e54dc0cee51</td>\n    </tr>\n    <tr>\n      <th>1056</th>\n      <td>./CAPTUM/Omalizumab/Cyclosporine/Koski 2017.pdf</td>\n      <td>6cfd7540663be0f6d7fb72f776339b71</td>\n    </tr>\n    <tr>\n      <th>1058</th>\n      <td>./CAPTUM/table final .pdf</td>\n      <td>f13be81ffbff55e031a34ef81d43cbff</td>\n    </tr>\n  </tbody>\n</table>\n<p>466 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "df_unique = df.drop_duplicates(subset=['checksum'])\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to read the text from the pdfs.   \n",
    "First we import the neccessary modules (more on pdfminer.six [here](https://pdfminersix.readthedocs.io/en/latest/)) and create a procedure to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "def convert_pdf_to_string(file_path):\n",
    "\toutput_string = StringIO()\n",
    "\twith open(file_path, 'rb') as in_file:\n",
    "\t    parser = PDFParser(in_file)\n",
    "\t    doc = PDFDocument(parser)\n",
    "\t    rsrcmgr = PDFResourceManager()\n",
    "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\t    for page in PDFPage.create_pages(doc):\n",
    "\t        interpreter.process_page(page)\n",
    "\n",
    "\treturn(output_string.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-21-4f1570f4a70c>:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_unique['text'] = df_unique.filepath.apply(lambda fp: convert_pdf_to_string(fp))\n"
     ]
    }
   ],
   "source": [
    "# this takes between five and ten minutes\n",
    "df_unique['text'] = df_unique.filepath.apply(lambda fp: convert_pdf_to_string(fp))\n",
    "df_unique.to_csv('captum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we iterate over each individual file and use pdfminer to get the file content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/466 done\n",
      "20/466 done\n",
      "30/466 done\n",
      "50/466 done\n",
      "70/466 done\n",
      "80/466 done\n",
      "90/466 done\n",
      "100/466 done\n",
      "110/466 done\n",
      "130/466 done\n",
      "140/466 done\n",
      "150/466 done\n",
      "160/466 done\n",
      "170/466 done\n",
      "180/466 done\n",
      "190/466 done\n",
      "200/466 done\n",
      "220/466 done\n",
      "230/466 done\n",
      "310/466 done\n",
      "330/466 done\n",
      "340/466 done\n",
      "350/466 done\n",
      "360/466 done\n",
      "370/466 done\n",
      "400/466 done\n",
      "450/466 done\n",
      "480/466 done\n",
      "500/466 done\n",
      "510/466 done\n",
      "520/466 done\n",
      "560/466 done\n",
      "580/466 done\n",
      "590/466 done\n",
      "600/466 done\n",
      "630/466 done\n",
      "640/466 done\n",
      "660/466 done\n",
      "720/466 done\n",
      "730/466 done\n",
      "750/466 done\n",
      "760/466 done\n",
      "780/466 done\n",
      "790/466 done\n",
      "860/466 done\n",
      "900/466 done\n",
      "930/466 done\n",
      "950/466 done\n",
      "970/466 done\n"
     ]
    }
   ],
   "source": [
    "# Remove Output Path if it exists, then create new\n",
    "import shutil\n",
    "out_dir = \"./out/\"\n",
    "if os.path.exists(out_dir):\n",
    "    shutil.rmtree(out_dir)\n",
    "os.makedirs(out_dir)\n",
    "\n",
    "# Iterate over each individual file\n",
    "for index, row in df_unique.iterrows():\n",
    "    path = row['filepath']\n",
    "    checksum = row['checksum']\n",
    "\n",
    "    content = convert_pdf_to_string(path)\n",
    "    \n",
    "    out_file_path = os.path.join(out_dir, checksum + \".txt\")\n",
    "    \n",
    "    with open(out_file_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(content)\n",
    "        \n",
    "    if ((index + 1) % 10 == 0):\n",
    "        print(str(index + 1) + \"/\" + str(len(df_unique)) + \" done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}